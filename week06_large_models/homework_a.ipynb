{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies: a recent pytorch and %pip install transformers==4.38.1 datasets==2.17.1 accelerate==0.27.2\n",
    "# the assignment will still likely work on other recent versions (transformers>4.25 and datasets>2.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMJ9Na9HMAK2"
   },
   "source": [
    "\n",
    "### Option A: Memory-efficient training and inference\n",
    "\n",
    "![img](https://steamuserimages-a.akamaihd.net/ugc/280721626864094662/C48355EE16889197B8D000A198F970CD7E64CB7A/?imw=512&imh=342&ima=fit&impolicy=Letterbox&imcolor=%23000000&letterbox=true)\n",
    "\n",
    "__Your quest__ is to fine-tune a large language with restricted GPU memory. You can choose one of these two models:\n",
    "\n",
    "- colab, kaggle or datasphere: [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)\n",
    "- if you have >64GB disk space: [facebook/opt-iml-30b](https://huggingface.co/facebook/opt-iml-max-30b)\n",
    "\n",
    "Both are powerful language models: opt-6.7b was trained to generate text and opt-iml was trained to follow human instructions.\n",
    "\n",
    "You can use __up to 10GiB GPU memory__ (as in 3080 or 2080Ti) for 6.7B model and up to 48GB for the 30B one. We deliberately limit GPU memory below and recommend you to check the peak memory usage via: [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html). We shall also assume that you don't have enough RAM to load the full model on CPU. If your your machine has enough, you may take advantage of it.\n",
    "\n",
    "\n",
    "Your code should be able to do 3 things:\n",
    "* run forward pass on a sequence of 2048 tokens\n",
    "* compute gradients w.r.t. a small subset of parameters: only one layer or similar\n",
    "* generate an answer to a question using `model.generate` (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEt3S33OL-y-",
    "outputId": "b06ca965-96b9-447a-b2ae-7ae60eee6d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting memory limit to 13.89%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# if your GPU has less than 10GB memory, please remove the code below\n",
    "# if your GPU has less than 4GB memory, use colab or kaggle instead\n",
    "max_memory_gib = torch.cuda.get_device_properties('cuda').total_memory / 2 ** 30\n",
    "torch.cuda.set_per_process_memory_fraction(min(1.0, 10 / max_memory_gib))\n",
    "print(f\"Setting memory limit to {min(1.0, 11 / max_memory_gib) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're gonna load a smaller version of the model to show you around.\n",
    "\n",
    "The large models use the same code, but with more layers & hidden units - so you can debug your code on the smaller model, then switch to the real deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sTuoIY_tNSVk"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "model_name = \"facebook/opt-iml-1.3b\"   # full model: 'facebook/opt-6.7b' or facebook/opt-iml-30b\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference baseline\n",
    "\n",
    "Here's a simple code that generates some tokens without offloading. You can use this as a reference, to check that your offloading algorithm is correct. Naturally, it will not work on the full 6.7B (or 30B) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEYYK1clPm-_",
    "outputId": "8e63b609-3021-4552-dd37-56db187eb8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: {'input_ids': tensor([[    2,   250,  4758,  4005],         [    2, 41  ...\n"
     ]
    }
   ],
   "source": [
    "# here's how the model works: tokenizer converts raw data to pytorch tensors\n",
    "batch = tokenizer([\"A cat sat\", \"import numpy\"], return_tensors='pt')\n",
    "batch = {name: tensor.cuda() for name, tensor in batch.items()}\n",
    "print(\"Batch:\", repr(batch)[:70].replace('\\n', ' '), ' ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWYjNSsLOFX6",
    "outputId": "66d93537-b72a-44b4-d402-58619f420aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample A: </s>A cat sat on my lap and I was watching a movie. I was about to fall asleep and the cat jumped up and started licking my face. I\n",
      "Sample B: </s>import numpy.array\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import numpy.array\n"
     ]
    }
   ],
   "source": [
    "# fun fact: you can use the model to generate text given prefix\n",
    "generated_ids = model.generate(**batch, max_length=32)\n",
    "print(\"Sample A:\", tokenizer.decode(generated_ids[0]))\n",
    "print(\"Sample B:\", tokenizer.decode(generated_ids[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training baseline\n",
    "\n",
    "Here's some sample data you can use for prototyping -- and demonstrating that your algorithm works.\n",
    "Then again, you are free to use any dataset you like.\n",
    "\n",
    "We also provide a very simple fine-tuning example that mimics [BitFit](https://arxiv.org/abs/2106.10199)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "da5472c281214cf5bd97fa38b476d8e8",
      "710f693325604d8cabeb3c17e79940b6",
      "71068694e7b94a66a9f152ada29870e6",
      "98714743216849458471ab4758350240",
      "aba1c0d2c8a7442db243b8046f0ea99f",
      "5b6ef40ef59041dd98ea18c3debf5d54",
      "f7e122b6821c4dd8b819c4d8bba5142b",
      "d33367c7059645f5a78771ac8047c48f",
      "5778fe60169b423981a78309337f64ca",
      "d666e06bbf364b2fa9c45174c29aea44",
      "99d72ded561d45b2b04f1c0918157a52"
     ]
    },
    "id": "i0kndblQWVt9",
    "outputId": "77f58a58-7320-4946-f6d6-d251a7dc87fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/jheuristic/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f060e11b0e41b6bfd26ee9102ce209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2354: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikitext\", \"wikitext-2-v1\")['train']\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sample_batch = tokenizer(data['text'][:1], max_length=5, padding=True, pad_to_multiple_of=5, return_tensors='pt')\n",
    "\n",
    "# note: sample_batch has a size of 1x5, you will need a larger batch in the next assignment\n",
    "# note(2) if you want something more peculiar, https://huggingface.co/datasets/transformersbook/codeparrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1315.76 million\n",
      "Trained parameters: 0.54 million\n",
      "Loss[0] = 0.011\n",
      "Loss[1] = 0.010\n",
      "Loss[2] = 0.009\n",
      "Loss[3] = 0.008\n",
      "Loss[4] = 0.007\n",
      "Loss[5] = 0.007\n",
      "Loss[6] = 0.006\n",
      "Loss[7] = 0.005\n",
      "Loss[8] = 0.005\n",
      "Loss[9] = 0.004\n"
     ]
    }
   ],
   "source": [
    "# example: only train bias parameters, as in the BitFitPaper\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = name.endswith(\"bias\")\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.to(torch.float32)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:0.2f} million\")\n",
    "print(f\"Trained parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:0.2f} million\")\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# model turns those tensors into logits (pre-softmax activations) and loss\n",
    "# in the example below, logits are available as pred.logits, and loss is pred.loss\n",
    "\n",
    "for i in range(10):\n",
    "    sample_batch = {name: tensor.cuda() for name, tensor in sample_batch.items()}\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = model(**sample_batch, labels=sample_batch['input_ids']).loss / 1000\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(f\"Loss[{i}] = {loss.item():.3f}\")\n",
    "\n",
    "# if all went well, you'll see the loss go down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it looked a bit too easy - that was because you are dealing with a small model that fits into RAM. Once you have something larger, you can no longer simply `.from_pretrained` your model. Instead, you will need to process weights in small groups - the way they are stored in Hugging Face hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQdgK3ohSyN3"
   },
   "source": [
    "## Quest 1 details\n",
    "\n",
    "\n",
    "Your main objective is to implement parameter offloading and solve two problems: fine-tuning and inference.\n",
    "\n",
    "__Task 1.1:__ run forward and backward pass, accumulate gradients w.r.t. a subset of model parameters. Use a training batch size of 128 sequences, and sequence length of 1024 tokens. In other words, `input_ids.shape == (128, 1024)`.\n",
    "\n",
    "You may choose one of these options:\n",
    "- train only the embedding layer, [similar to this paper](https://arxiv.org/abs/2104.08691)\n",
    "- train only the bias parameters from Linear and LayerNorm, [like this paper](https://arxiv.org/abs/2106.10199)\n",
    "- train low-rank adapters, [like in this paper](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "You don't have to train the model to convergence, just show that it can run 10 consecutive forward-backward-step passes and **the loss goes down**. You can even run those forward/backward passes on the same batch!\n",
    "\n",
    "__Task 1.2:__ generate a short sequence given a prefix. You may choose any generation task that requires generating at least 25 consecutive tokens. Here's one example from the NLP course (the generated code is in blue)\n",
    "\n",
    "![image.png](https://camo.githubusercontent.com/9d2e12db3d8afd576baca7637f9787802498a45beda018efb55a453a4a69e2aa/68747470733a2f2f692e696d6775722e636f6d2f613151684b46372e706e67)\n",
    "\n",
    "You may use model.generate (if your code is compatible with that) or write your own inference loop. If you choose to write your own loop, you are free to use sampling, greedy, top-p, top-k or any other [inference mode supported by HF transformers](https://huggingface.co/docs/transformers/main_classes/text_generation).\n",
    "\n",
    "\n",
    "__Grading (10 points):__\n",
    "\n",
    "- __+2 points__ you can perform forward pass with offloading on *some* input sequence (any batch size / length)\n",
    "- __+2 points__ check that forward pass with offloading is `torch.allclose` to forward pass without offloading\n",
    "    - since you (likely) can't run the full model w/o offloading, test it the 1.3B model from earlier\n",
    "- __+2 point__ you can perform forward pass on 128x1024 tokens of actual text data (e.g. the sample data above)\n",
    "- __+2 point__ you can compute gradients with offloading on the same 128x1024 tokens from the real text data\n",
    "- __+2 point__ you can run inference of the model - and it generates some human-readable text\n",
    "- __bonus points__ optimize your code so that it would pre-load the next offloaded layer in background\n",
    "\n",
    "__Conditions:__\n",
    "- using more than 10GiB of GPU memory at any point is forbidden (check with [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html))\n",
    "- please keep all model parameters in either float16, bfloat16, or float32 - no quantization for now\n",
    "   - if you *really* want to show off quantization, evaluate your code with both original and quantized weights\n",
    "- at least 99% of model's floating point computations should be done on GPU. If you find a server with a ton of RAM and run the model on cpu, it will not count as a solution\n",
    "- please do **not** use any thrid-party offloading implementations (e.g. from deepspeed or accelerate)\n",
    "- your solution may be slow - especially when loading from Colab disks. This is not your fault :)\n",
    "   - if you found a way to speed up the code in a non-trivial way (e.g. load i+1st layer in parallel when computing i-th), please attach a short summary of what when submitting the notebook (e.g. anytask/lms) to get bonus points\n",
    "\n",
    "\n",
    "\n",
    "__FAQ:__\n",
    "\n",
    "- __I am getting out-of-memory errors for no reason!__\n",
    "  - it could be because of some leftover tensors from previous cells. To get rid of them, please restart the notebook and only run the code that is relevant to your current task.\n",
    "\n",
    "- __The forward pass activations are too large, it does not fit!__\n",
    "   - __Gradient accumulation:__ you probably can't process 128 sequences at once -- but what if you accumulate them over several forward/batckward passes with a smaller batch size.\n",
    "   - __Gradient checkpointing:__ you can further reduce activation memory by not storing intermediate activations. You can learn how to usa built-in checkpoints [from their docs](https://huggingface.co/docs/transformers/main_classes/model) or build your own using [PyTorch default checkpointing](https://pytorch.org/docs/stable/checkpoint.html).\n",
    "  \n",
    "- __My float16 gradients are NaN!__\n",
    "   - There should be a way to scale your loss function by a constant -- only to un-scale it later. You can use GradScaler from [PyTorch AMP](https://pytorch.org/docs/stable/amp.html) or write your own monstrosity.\n",
    "   - You can also cast weights to bfloat16 _but you have to demonstrate that bfloat16 model generates the same (or close) output as float16 one!_ As in \"you have to write a short report with code and samples.\"\n",
    "     \n",
    "- __I can run forward with no_grad, but running with grad goes out of memory!__\n",
    "   - If the problem only occurs with large batches, please see \"activations are too large\" above.\n",
    "   - If you get OOM errors even with a single training token (a 1x1 batch), but only in training mode,\n",
    "     maybe you forgot to mark most parameters as `requires_grad=False`? The .grad buffers can be quite large.\n",
    "     \n",
    "   - If not, OOM  be because PyTorch autograd remembers the intermediate weight tensors for backprop.\n",
    "     For example, consider this code:\n",
    "     \n",
    "```python\n",
    "    x = embeddings_and_input_layernorm(input_ids)\n",
    "    for layer_index in range(num_layers):\n",
    "        layer = load_from_disk(layer_index)\n",
    "        x = layer(x)\n",
    "        del layer  # we no longer need this layer's weights, but PyTorch will keep it in memory for autograd!\n",
    "```\n",
    "\n",
    "    If this is your case, you can write an autograd function that loads the necessary weight.\n",
    "    a look at \"[Optional] Suggested Interface\" section below.\n",
    "     \n",
    "- __I cannot load the full model even in CPU RAM!__\n",
    "   - This is intended - and a real problem that you often face in production.\n",
    "     You gotta find a way to prepare your model for offloading without loading the full thing into RAM.\n",
    "     In the next section, we explain how you can handle checkpoints and initialize the model in google colab.\n",
    "     Please see the [Optional] sections that mention low RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> <h3> <u> [Optional] Suggested Interface with torch.autograd.Function (click to expand) </u> </h3> </summary>\n",
    "\n",
    "You can assume that offloaded weights do not require grad themselves - but they take part in intermediate computations that *do* require grad.\n",
    "The problem is, if you load weights naively without `torch.no_grad`, PyTorch will remember them until the end of backward pass. If not addressed, this will keep all model weights in memory and mess up your offloading.\n",
    "\n",
    "\n",
    "To avoid this, you can implement a custom autograd function that loads weights from ram / disk internally. That way, PyTorch will not keep any gpu tensors except unless you explicitly tell it to. Crucially, __we only need this function for linear layers__ since all other layers can fit on GPU. Though, you may *optionally* offload embedding layers as well.\n",
    "\n",
    "\n",
    "Here's [some documentation](https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd) on writing your own autograd functions. Your solution could look something like this:\n",
    "\n",
    "\n",
    "```python\n",
    "class _OffloadedLinearOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, saved_weight_path, bias_or_none):\n",
    "        weight = you.load_by_name(saved_weight_path)\n",
    "        ctx._saved_weight_path = saved_weight_path\n",
    "        ctx._has_bias = bias_or_none is not None\n",
    "        return torch.nn.functional.linear(input, weight, bias=bias_or_none)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weight = you.load_by_name(ctx._saved_weight_path)\n",
    "        grad_input = torch.nn.functional.linear(grad_output, weight.t())\n",
    "        grad_bias = grad_output.flatten(0, -2).sum(0) if ctx._has_bias else None\n",
    "        return grad_input, None, grad_bias\n",
    "\n",
    "    \n",
    "# to use:\n",
    "# output = _OffloadedLinearOp.apply(input, \"my_weight.pth\", bias)\n",
    "# loss(output).backward()  # uses custom backward\n",
    "```\n",
    "\n",
    "You can implement this function separately and test it on a single layer to make sure forward and backward passes match. Once you are confident in your code, it's time to apply it to your model. One way to do this is:\n",
    "\n",
    "\n",
    "```python\n",
    "class MyOffloadedLinear(torch.nn.Module):\n",
    "    def __init__(self, saved_weight_path, bias_or_none):\n",
    "        super().__init__()\n",
    "        self.saved_weight_path, self.bias_or_none = saved_weight_path, bias_or_none\n",
    "    def forward(self, input):\n",
    "        return _OffloadedLinearOp.apply(input, self.saved_weight_path, self.bias_or_none)\n",
    "\n",
    "for module_that_contains_linear in you.find_these_modules(model):\n",
    "    linear = you.take_linear_layer_from(module_that_contains_linear)\n",
    "    saved_weight_path = save_weight_somewhere(linear.weight)\n",
    "    offloaded_linear = MyOffloadedLinear(saved_weight_path, linear.bias)\n",
    "    you.replace_that_linear_with(offloaded_linear)\n",
    "```\n",
    "    \n",
    "Please note that this algorithm is \"lazy\" in the sense that it loads weights just in time. A smarter (and faster!) way to offload the data is to do it in parallel: once you load the first weight, you immediately start loading the second weight from disk in a background thread. You can do this by recording the order in which your model uses the offloaded weights and keeping track of which weight you should load next.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><h3><u>[Optional] How to initialize the model with low RAM (click to expand)</u></h3></summary>\n",
    "    \n",
    "    The trick is that you don't initialize all modules at once.\n",
    "    Instead, you can load *some* modules, prepare them for offloading (e.g. remove some params), then load the next bunch of modules.\n",
    "    \n",
    "    Here's one way you can do this:\n",
    "\n",
    "    ```python\n",
    "    config = transformers.AutoConfig.from_pretrained(\"facebook/opt-6.7b\")\n",
    "    actual_hidden_layers = config.num_hidden_layers\n",
    "    config.num_hidden_layers = 0  # create a model with no hidden layers\n",
    "    model = transformers.AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)\n",
    "    print(f\"Total parameters (embeddings only): {sum(p.numel() for p in model.parameters())/1e6:0.2f} million\")\n",
    "    # only 0.21 billion instead of 6.7\n",
    "\n",
    "    for _ in range(actual_hidden_layers):\n",
    "        new_layer = transformers.models.opt.modeling_opt.OPTDecoderLayer(config)\n",
    "        new_layer = you.prepare_for_offloading(new_layer)\n",
    "        model.model.decoder.layers.append(new_layer)\n",
    "    config.num_hidden_layers = actual_hidden_layers\n",
    "\n",
    "    you.load_parameters_that_werent_offloaded(model, preprocessed_checkpoint_chunks)\n",
    "    ```\n",
    "    \n",
    "    If `you.prepare_for_offloading` properly offloads all heavy parameters to the disk, this code will build the full offloaded model without going over 10GB CPU RAM.\n",
    "    We also recommend that you check that the resulting code works correctly by test-running it on the 1.3B model.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><h3><u>[Optional] Dealing with HuggingFace weights with low RAM (click to expand)</u></h3></summary>\n",
    "\n",
    "\n",
    "When you download a Hugging Face model, there will be one or more \"chunks\", holding the data parameters. These chunks can be seen in the model repository, under \"Files and versions\" tab:\n",
    "![image.png](https://i.imgur.com/3gZ2KPB.png)\n",
    "\n",
    "Reference links to \"files and versions\": [opt-6.7b](https://huggingface.co/facebook/opt-6.7b/tree/main), [opt-iml-30b](https://huggingface.co/facebook/opt-iml-30b/tree/main)\n",
    "\n",
    "You can download individual chunks of parameters by going clicking on a chunk and copying the \"download\" url, like this:\n",
    "\n",
    "![img](https://i.imgur.com/cv9WvYw.png)\n",
    "\n",
    "Any chunks downloaded this way will contain a `torch.load`-able state dict. Here's how it works:\n",
    "\n",
    "```python\n",
    "# example: download one (small) chunk out of OPT-IML-30B \n",
    "chunk7_download_url = \"https://huggingface.co/facebook/opt-iml-30b/resolve/828fabfb08d5d3f81b4d33cd27a64e3a360a5770/pytorch_model-00007-of-00007.bin\"\n",
    "!wget {chunk7_download_url} -O \"chunk7.pth\"\n",
    "\n",
    "partial_state_dict = torch.load(\"chunk7.pth\")\n",
    "print(f\"Keys:\", partial_state_dict.keys(), '\\n')\n",
    "print(f\"Shape of decoder.layers.47.fc1.weight: {partial_state_dict['decoder.layers.47.fc1.weight'].shape}\")\n",
    "# Keys: dict_keys(['decoder.layers.47.fc1.weight', 'decoder.layers.47.fc1.bias', 'decoder.layers.47.fc2.weight', 'decoder.layers.47.fc2.bias', 'decoder.layers.47.final_layer_norm.weight', 'decoder.layers.47.final_layer_norm.bias']) \n",
    "# Shape of decoder.layers.47.fc1.weight: torch.Size([28672, 7168])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CR2KrOtGgz9D"
   },
   "outputs": [],
   "source": [
    "# if it helps, <YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPKl2dqHxx9eVooNA2VFgNG",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5778fe60169b423981a78309337f64ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b6ef40ef59041dd98ea18c3debf5d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71068694e7b94a66a9f152ada29870e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7e122b6821c4dd8b819c4d8bba5142b",
      "placeholder": "​",
      "style": "IPY_MODEL_5b6ef40ef59041dd98ea18c3debf5d54",
      "value": "100%"
     }
    },
    "710f693325604d8cabeb3c17e79940b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98714743216849458471ab4758350240": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5778fe60169b423981a78309337f64ca",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d33367c7059645f5a78771ac8047c48f",
      "value": 3
     }
    },
    "99d72ded561d45b2b04f1c0918157a52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aba1c0d2c8a7442db243b8046f0ea99f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d72ded561d45b2b04f1c0918157a52",
      "placeholder": "​",
      "style": "IPY_MODEL_d666e06bbf364b2fa9c45174c29aea44",
      "value": " 3/3 [00:00&lt;00:00, 18.51it/s]"
     }
    },
    "d33367c7059645f5a78771ac8047c48f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d666e06bbf364b2fa9c45174c29aea44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da5472c281214cf5bd97fa38b476d8e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71068694e7b94a66a9f152ada29870e6",
       "IPY_MODEL_98714743216849458471ab4758350240",
       "IPY_MODEL_aba1c0d2c8a7442db243b8046f0ea99f"
      ],
      "layout": "IPY_MODEL_710f693325604d8cabeb3c17e79940b6"
     }
    },
    "f7e122b6821c4dd8b819c4d8bba5142b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
